{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School Documnets\\Internship 2\\Employee_onboarding_chatbot\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Now let's implement BERT with the correct column names\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from extended_function import *\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/combine_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and prepare data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare the data with correct column names\n",
    "messages = df['Employee_message'].apply(clean_message)\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize with smaller max_length\n",
    "max_length = 64\n",
    "encodings = tokenizer(\n",
    "    messages.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "labels_tensor = torch.tensor(labels)\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels_tensor)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders with smaller batch size\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1, Batch 10/55, Loss: 2.3675\n",
      "Epoch 1, Batch 20/55, Loss: 2.2729\n",
      "Epoch 1, Batch 30/55, Loss: 2.2311\n",
      "Epoch 1, Batch 40/55, Loss: 2.2491\n",
      "Epoch 1, Batch 50/55, Loss: 2.0678\n",
      "Epoch 1 average loss: 2.2046\n",
      "Epoch 2, Batch 10/55, Loss: 2.0310\n",
      "Epoch 2, Batch 20/55, Loss: 2.0646\n",
      "Epoch 2, Batch 30/55, Loss: 1.8512\n",
      "Epoch 2, Batch 40/55, Loss: 1.8567\n",
      "Epoch 2, Batch 50/55, Loss: 1.8521\n",
      "Epoch 2 average loss: 1.9443\n",
      "Epoch 3, Batch 10/55, Loss: 1.6054\n",
      "Epoch 3, Batch 20/55, Loss: 1.6912\n",
      "Epoch 3, Batch 30/55, Loss: 1.6055\n",
      "Epoch 3, Batch 40/55, Loss: 1.5910\n",
      "Epoch 3, Batch 50/55, Loss: 1.4940\n",
      "Epoch 3 average loss: 1.5590\n",
      "Epoch 4, Batch 10/55, Loss: 1.0497\n",
      "Epoch 4, Batch 20/55, Loss: 1.3648\n",
      "Epoch 4, Batch 30/55, Loss: 0.9909\n",
      "Epoch 4, Batch 40/55, Loss: 1.0097\n",
      "Epoch 4, Batch 50/55, Loss: 0.8394\n",
      "Epoch 4 average loss: 1.1207\n",
      "Epoch 5, Batch 10/55, Loss: 0.7479\n",
      "Epoch 5, Batch 20/55, Loss: 0.7670\n",
      "Epoch 5, Batch 30/55, Loss: 0.9787\n",
      "Epoch 5, Batch 40/55, Loss: 1.0514\n",
      "Epoch 5, Batch 50/55, Loss: 0.7693\n",
      "Epoch 5 average loss: 0.7661\n",
      "Epoch 6, Batch 10/55, Loss: 0.5082\n",
      "Epoch 6, Batch 20/55, Loss: 0.5627\n",
      "Epoch 6, Batch 30/55, Loss: 0.4042\n",
      "Epoch 6, Batch 40/55, Loss: 0.4478\n",
      "Epoch 6, Batch 50/55, Loss: 0.6148\n",
      "Epoch 6 average loss: 0.5319\n",
      "Epoch 7, Batch 10/55, Loss: 0.3100\n",
      "Epoch 7, Batch 20/55, Loss: 0.4461\n",
      "Epoch 7, Batch 30/55, Loss: 0.3033\n",
      "Epoch 7, Batch 40/55, Loss: 0.2542\n",
      "Epoch 7, Batch 50/55, Loss: 0.3925\n",
      "Epoch 7 average loss: 0.3748\n",
      "Epoch 8, Batch 10/55, Loss: 0.2095\n",
      "Epoch 8, Batch 20/55, Loss: 0.3402\n",
      "Epoch 8, Batch 30/55, Loss: 0.2511\n",
      "Epoch 8, Batch 40/55, Loss: 0.2843\n",
      "Epoch 8, Batch 50/55, Loss: 0.5175\n",
      "Epoch 8 average loss: 0.2723\n",
      "Epoch 9, Batch 10/55, Loss: 0.2007\n",
      "Epoch 9, Batch 20/55, Loss: 0.3638\n",
      "Epoch 9, Batch 30/55, Loss: 0.1999\n",
      "Epoch 9, Batch 40/55, Loss: 0.1766\n",
      "Epoch 9, Batch 50/55, Loss: 0.2130\n",
      "Epoch 9 average loss: 0.2081\n",
      "Epoch 10, Batch 10/55, Loss: 0.1832\n",
      "Epoch 10, Batch 20/55, Loss: 0.1966\n",
      "Epoch 10, Batch 30/55, Loss: 0.0936\n",
      "Epoch 10, Batch 40/55, Loss: 0.1069\n",
      "Epoch 10, Batch 50/55, Loss: 0.4304\n",
      "Epoch 10 average loss: 0.1732\n",
      "Epoch 11, Batch 10/55, Loss: 0.1183\n",
      "Epoch 11, Batch 20/55, Loss: 0.1410\n",
      "Epoch 11, Batch 30/55, Loss: 0.1341\n",
      "Epoch 11, Batch 40/55, Loss: 0.2613\n",
      "Epoch 11, Batch 50/55, Loss: 0.0773\n",
      "Epoch 11 average loss: 0.1313\n",
      "Epoch 12, Batch 10/55, Loss: 0.1291\n",
      "Epoch 12, Batch 20/55, Loss: 0.3894\n",
      "Epoch 12, Batch 30/55, Loss: 0.0841\n",
      "Epoch 12, Batch 40/55, Loss: 0.1031\n",
      "Epoch 12, Batch 50/55, Loss: 0.1260\n",
      "Epoch 12 average loss: 0.1171\n",
      "Epoch 13, Batch 10/55, Loss: 0.0836\n",
      "Epoch 13, Batch 20/55, Loss: 0.0947\n",
      "Epoch 13, Batch 30/55, Loss: 0.0673\n",
      "Epoch 13, Batch 40/55, Loss: 0.0532\n",
      "Epoch 13, Batch 50/55, Loss: 0.0618\n",
      "Epoch 13 average loss: 0.0922\n",
      "Epoch 14, Batch 10/55, Loss: 0.0851\n",
      "Epoch 14, Batch 20/55, Loss: 0.0824\n",
      "Epoch 14, Batch 30/55, Loss: 0.1205\n",
      "Epoch 14, Batch 40/55, Loss: 0.0839\n",
      "Epoch 14, Batch 50/55, Loss: 0.0591\n",
      "Epoch 14 average loss: 0.0704\n",
      "Epoch 15, Batch 10/55, Loss: 0.0743\n",
      "Epoch 15, Batch 20/55, Loss: 0.0839\n",
      "Epoch 15, Batch 30/55, Loss: 0.0464\n",
      "Epoch 15, Batch 40/55, Loss: 0.0871\n",
      "Epoch 15, Batch 50/55, Loss: 0.0681\n",
      "Epoch 15 average loss: 0.0660\n",
      "Epoch 16, Batch 10/55, Loss: 0.0599\n",
      "Epoch 16, Batch 20/55, Loss: 0.0423\n",
      "Epoch 16, Batch 30/55, Loss: 0.0652\n",
      "Epoch 16, Batch 40/55, Loss: 0.0457\n",
      "Epoch 16, Batch 50/55, Loss: 0.0412\n",
      "Epoch 16 average loss: 0.0577\n",
      "Epoch 17, Batch 10/55, Loss: 0.0561\n",
      "Epoch 17, Batch 20/55, Loss: 0.0488\n",
      "Epoch 17, Batch 30/55, Loss: 0.0329\n",
      "Epoch 17, Batch 40/55, Loss: 0.0495\n",
      "Epoch 17, Batch 50/55, Loss: 0.0530\n",
      "Epoch 17 average loss: 0.0478\n",
      "Epoch 18, Batch 10/55, Loss: 0.0366\n",
      "Epoch 18, Batch 20/55, Loss: 0.0394\n",
      "Epoch 18, Batch 30/55, Loss: 0.0312\n",
      "Epoch 18, Batch 40/55, Loss: 0.0352\n",
      "Epoch 18, Batch 50/55, Loss: 0.0358\n",
      "Epoch 18 average loss: 0.0478\n",
      "Epoch 19, Batch 10/55, Loss: 0.0371\n",
      "Epoch 19, Batch 20/55, Loss: 0.0347\n",
      "Epoch 19, Batch 30/55, Loss: 0.0401\n",
      "Epoch 19, Batch 40/55, Loss: 0.0339\n",
      "Epoch 19, Batch 50/55, Loss: 0.0383\n",
      "Epoch 19 average loss: 0.0433\n",
      "Epoch 20, Batch 10/55, Loss: 0.0369\n",
      "Epoch 20, Batch 20/55, Loss: 0.0371\n",
      "Epoch 20, Batch 30/55, Loss: 0.0238\n",
      "Epoch 20, Batch 40/55, Loss: 0.0404\n",
      "Epoch 20, Batch 50/55, Loss: 0.0274\n",
      "Epoch 20 average loss: 0.0353\n",
      "Epoch 21, Batch 10/55, Loss: 0.0241\n",
      "Epoch 21, Batch 20/55, Loss: 0.0327\n",
      "Epoch 21, Batch 30/55, Loss: 0.0360\n",
      "Epoch 21, Batch 40/55, Loss: 0.0377\n",
      "Epoch 21, Batch 50/55, Loss: 0.0263\n",
      "Epoch 21 average loss: 0.0347\n",
      "Epoch 22, Batch 10/55, Loss: 0.0225\n",
      "Epoch 22, Batch 20/55, Loss: 0.0258\n",
      "Epoch 22, Batch 30/55, Loss: 0.0341\n",
      "Epoch 22, Batch 40/55, Loss: 0.0293\n",
      "Epoch 22, Batch 50/55, Loss: 0.0205\n",
      "Epoch 22 average loss: 0.0364\n",
      "Epoch 23, Batch 10/55, Loss: 0.0188\n",
      "Epoch 23, Batch 20/55, Loss: 0.0293\n",
      "Epoch 23, Batch 30/55, Loss: 0.0375\n",
      "Epoch 23, Batch 40/55, Loss: 0.0349\n",
      "Epoch 23, Batch 50/55, Loss: 0.0221\n",
      "Epoch 23 average loss: 0.0269\n",
      "Epoch 24, Batch 10/55, Loss: 0.0225\n",
      "Epoch 24, Batch 20/55, Loss: 0.0198\n",
      "Epoch 24, Batch 30/55, Loss: 0.0230\n",
      "Epoch 24, Batch 40/55, Loss: 0.0234\n",
      "Epoch 24, Batch 50/55, Loss: 0.0171\n",
      "Epoch 24 average loss: 0.0232\n",
      "Epoch 25, Batch 10/55, Loss: 0.0169\n",
      "Epoch 25, Batch 20/55, Loss: 0.0194\n",
      "Epoch 25, Batch 30/55, Loss: 0.0190\n",
      "Epoch 25, Batch 40/55, Loss: 0.0225\n",
      "Epoch 25, Batch 50/55, Loss: 0.0157\n",
      "Epoch 25 average loss: 0.0209\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training loop with early stopping\n",
    "model.train()\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):  \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device).long()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Print every 10 batches\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...\n",
      "\n",
      "Saving complete.\n"
     ]
    }
   ],
   "source": [
    "# Save the whole model\n",
    "print(\"\\nSaving model...\")\n",
    "torch.save(model, \"../model/model_25epochs.pth\")\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"../model/model_state_25epochs.pth\")\n",
    "\n",
    "# Assuming 'model' is your model and 'optimizer' is your optimizer\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "}\n",
    "# torch.save(checkpoint, '../model/checkpoint.pth')\n",
    "\n",
    "print(\"\\nSaving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n",
      "Test Accuracy: 0.9545\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"\\nEvaluating model...\")\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model...\n",
      "\n",
      "Model Load Completed\n"
     ]
    }
   ],
   "source": [
    "# Load saved model\n",
    "print(\"\\nLoading model...\")\n",
    "model = torch.load(\"../model/model_25epochs.pth\", weights_only=False)\n",
    "print(\"\\nModel Load Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict intent using the BERT model\n",
    "def predict_intent_bert(text):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "        predicted_label = label_encoder.inverse_transform([predicted.cpu().item()])[0]\n",
    "        \n",
    "    return predicted_label, confidence.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model with sample messages:\n",
      "--------------------------------------------------\n",
      "Message: I need to request time off for next week\n",
      "Predicted Intent: time_off_report\n",
      "Confidence: 0.99\n",
      "--------------------------------------------------\n",
      "Message: My computer keeps crashing and I can't work\n",
      "Predicted Intent: it_issue_report\n",
      "Confidence: 0.97\n",
      "--------------------------------------------------\n",
      "Message: I want to set up training for my team\n",
      "Predicted Intent: training_request\n",
      "Confidence: 0.63\n",
      "--------------------------------------------------\n",
      "Message: I need access to the sales database\n",
      "Predicted Intent: access_request\n",
      "Confidence: 0.99\n",
      "--------------------------------------------------\n",
      "Message: I want to report the harm\n",
      "Predicted Intent: harassment_report\n",
      "Confidence: 0.92\n",
      "--------------------------------------------------\n",
      "Message: I would like to sign up for a training session on effective communication.\n",
      "Predicted Intent: training_request\n",
      "Confidence: 0.92\n",
      "--------------------------------------------------\n",
      "Message: Could you please do my performance review meeting?\n",
      "Predicted Intent: performance_review\n",
      "Confidence: 0.84\n",
      "--------------------------------------------------\n",
      "Message: I need access to the shared project drive; can you help me out?\n",
      "Predicted Intent: access_request\n",
      "Confidence: 0.99\n",
      "--------------------------------------------------\n",
      "Message: I’m planning to relocate to the New York office; what is the process?\n",
      "Predicted Intent: relocation_request\n",
      "Confidence: 0.99\n",
      "--------------------------------------------------\n",
      "Message: I want to report a safety incident that occurred in the warehouse.\n",
      "Predicted Intent: safety_incident_report\n",
      "Confidence: 0.97\n",
      "--------------------------------------------------\n",
      "Message: I’d like to request time off for next month due to a personal commitment.\n",
      "Predicted Intent: time_off_report\n",
      "Confidence: 0.99\n",
      "--------------------------------------------------\n",
      "Message: Can you help me enroll in the company benefits program, specifically health insurance?\n",
      "Predicted Intent: benefits_enrollment\n",
      "Confidence: 0.99\n",
      "--------------------------------------------------\n",
      "Message: I need to report an incident of harassment that I witnessed at work.\n",
      "Predicted Intent: harassment_report\n",
      "Confidence: 0.98\n",
      "--------------------------------------------------\n",
      "Message: How do I set my performance goals for the next quarter?\n",
      "Predicted Intent: performance_review\n",
      "Confidence: 0.91\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model with sample messages\n",
    "test_messages = [\n",
    "    \"I need to request time off for next week\",\n",
    "    \"My computer keeps crashing and I can't work\",\n",
    "    \"I want to set up training for my team\",\n",
    "    \"I need access to the sales database\",\n",
    "    \"I want to report the harm\",\n",
    "    \"I would like to sign up for a training session on effective communication.\",\n",
    "    \"Could you please do my performance review meeting?\",\n",
    "    \"I need access to the shared project drive; can you help me out?\",\n",
    "    \"I’m planning to relocate to the New York office; what is the process?\",\n",
    "    \"I want to report a safety incident that occurred in the warehouse.\",\n",
    "    \"I’d like to request time off for next month due to a personal commitment.\",\n",
    "    \"Can you help me enroll in the company benefits program, specifically health insurance?\",\n",
    "    \"I need to report an incident of harassment that I witnessed at work.\",\n",
    "    \"How do I set my performance goals for the next quarter?\",\n",
    "    # \"My computer isn’t booting up properly; can you assist me with this issue?\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting model with sample messages:\")\n",
    "print(\"-\" * 50)\n",
    "for message in test_messages:\n",
    "    intent, confidence = predict_intent_bert(message)\n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Predicted Intent: {intent}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
