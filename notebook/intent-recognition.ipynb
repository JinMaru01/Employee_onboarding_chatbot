{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's implement BERT with the correct column names\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from extended_function import *\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/combine_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and prepare data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare the data with correct column names\n",
    "messages = df['Employee_message'].apply(clean_message)\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df['service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize with smaller max_length\n",
    "max_length = 64\n",
    "encodings = tokenizer(\n",
    "    messages.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "labels_tensor = torch.tensor(labels)\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels_tensor)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders with smaller batch size\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1, Batch 10/55, Loss: 2.4831\n",
      "Epoch 1, Batch 20/55, Loss: 2.4229\n",
      "Epoch 1, Batch 30/55, Loss: 2.4562\n",
      "Epoch 1, Batch 40/55, Loss: 2.2471\n",
      "Epoch 1, Batch 50/55, Loss: 2.0915\n",
      "Epoch 1 average loss: 2.2614\n",
      "Epoch 2, Batch 10/55, Loss: 1.9694\n",
      "Epoch 2, Batch 20/55, Loss: 2.1330\n",
      "Epoch 2, Batch 30/55, Loss: 1.8609\n",
      "Epoch 2, Batch 40/55, Loss: 2.0131\n",
      "Epoch 2, Batch 50/55, Loss: 1.8104\n",
      "Epoch 2 average loss: 1.9429\n",
      "Epoch 3, Batch 10/55, Loss: 1.6086\n",
      "Epoch 3, Batch 20/55, Loss: 1.5824\n",
      "Epoch 3, Batch 30/55, Loss: 1.5641\n",
      "Epoch 3, Batch 40/55, Loss: 1.5100\n",
      "Epoch 3, Batch 50/55, Loss: 1.4803\n",
      "Epoch 3 average loss: 1.5296\n",
      "Epoch 4, Batch 10/55, Loss: 1.0098\n",
      "Epoch 4, Batch 20/55, Loss: 1.1115\n",
      "Epoch 4, Batch 30/55, Loss: 1.0837\n",
      "Epoch 4, Batch 40/55, Loss: 0.8578\n",
      "Epoch 4, Batch 50/55, Loss: 0.8792\n",
      "Epoch 4 average loss: 1.0865\n",
      "Epoch 5, Batch 10/55, Loss: 1.0247\n",
      "Epoch 5, Batch 20/55, Loss: 0.7606\n",
      "Epoch 5, Batch 30/55, Loss: 0.5898\n",
      "Epoch 5, Batch 40/55, Loss: 0.4937\n",
      "Epoch 5, Batch 50/55, Loss: 0.8917\n",
      "Epoch 5 average loss: 0.7159\n",
      "Epoch 6, Batch 10/55, Loss: 0.5980\n",
      "Epoch 6, Batch 20/55, Loss: 0.4456\n",
      "Epoch 6, Batch 30/55, Loss: 0.5156\n",
      "Epoch 6, Batch 40/55, Loss: 0.3790\n",
      "Epoch 6, Batch 50/55, Loss: 0.4390\n",
      "Epoch 6 average loss: 0.4842\n",
      "Epoch 7, Batch 10/55, Loss: 0.2970\n",
      "Epoch 7, Batch 20/55, Loss: 0.2980\n",
      "Epoch 7, Batch 30/55, Loss: 0.2393\n",
      "Epoch 7, Batch 40/55, Loss: 0.4821\n",
      "Epoch 7, Batch 50/55, Loss: 0.2170\n",
      "Epoch 7 average loss: 0.3454\n",
      "Epoch 8, Batch 10/55, Loss: 0.2235\n",
      "Epoch 8, Batch 20/55, Loss: 0.3547\n",
      "Epoch 8, Batch 30/55, Loss: 0.4965\n",
      "Epoch 8, Batch 40/55, Loss: 0.4443\n",
      "Epoch 8, Batch 50/55, Loss: 0.2561\n",
      "Epoch 8 average loss: 0.2584\n",
      "Epoch 9, Batch 10/55, Loss: 0.1870\n",
      "Epoch 9, Batch 20/55, Loss: 0.3401\n",
      "Epoch 9, Batch 30/55, Loss: 0.1613\n",
      "Epoch 9, Batch 40/55, Loss: 0.1356\n",
      "Epoch 9, Batch 50/55, Loss: 0.1489\n",
      "Epoch 9 average loss: 0.1989\n",
      "Epoch 10, Batch 10/55, Loss: 0.1308\n",
      "Epoch 10, Batch 20/55, Loss: 0.1287\n",
      "Epoch 10, Batch 30/55, Loss: 0.1278\n",
      "Epoch 10, Batch 40/55, Loss: 0.2277\n",
      "Epoch 10, Batch 50/55, Loss: 0.1037\n",
      "Epoch 10 average loss: 0.1526\n",
      "Epoch 11, Batch 10/55, Loss: 0.1826\n",
      "Epoch 11, Batch 20/55, Loss: 0.1583\n",
      "Epoch 11, Batch 30/55, Loss: 0.0950\n",
      "Epoch 11, Batch 40/55, Loss: 0.1088\n",
      "Epoch 11, Batch 50/55, Loss: 0.0787\n",
      "Epoch 11 average loss: 0.1325\n",
      "Epoch 12, Batch 10/55, Loss: 0.1316\n",
      "Epoch 12, Batch 20/55, Loss: 0.0979\n",
      "Epoch 12, Batch 30/55, Loss: 0.1023\n",
      "Epoch 12, Batch 40/55, Loss: 0.0742\n",
      "Epoch 12, Batch 50/55, Loss: 0.0601\n",
      "Epoch 12 average loss: 0.1033\n",
      "Epoch 13, Batch 10/55, Loss: 0.1143\n",
      "Epoch 13, Batch 20/55, Loss: 0.1260\n",
      "Epoch 13, Batch 30/55, Loss: 0.0668\n",
      "Epoch 13, Batch 40/55, Loss: 0.3923\n",
      "Epoch 13, Batch 50/55, Loss: 0.0606\n",
      "Epoch 13 average loss: 0.0973\n",
      "Epoch 14, Batch 10/55, Loss: 0.0707\n",
      "Epoch 14, Batch 20/55, Loss: 0.1951\n",
      "Epoch 14, Batch 30/55, Loss: 0.0768\n",
      "Epoch 14, Batch 40/55, Loss: 0.0752\n",
      "Epoch 14, Batch 50/55, Loss: 0.0712\n",
      "Epoch 14 average loss: 0.0828\n",
      "Epoch 15, Batch 10/55, Loss: 0.0894\n",
      "Epoch 15, Batch 20/55, Loss: 0.0599\n",
      "Epoch 15, Batch 30/55, Loss: 0.0615\n",
      "Epoch 15, Batch 40/55, Loss: 0.0524\n",
      "Epoch 15, Batch 50/55, Loss: 0.0549\n",
      "Epoch 15 average loss: 0.0683\n",
      "Epoch 16, Batch 10/55, Loss: 0.0516\n",
      "Epoch 16, Batch 20/55, Loss: 0.0702\n",
      "Epoch 16, Batch 30/55, Loss: 0.0957\n",
      "Epoch 16, Batch 40/55, Loss: 0.0471\n",
      "Epoch 16, Batch 50/55, Loss: 0.0349\n",
      "Epoch 16 average loss: 0.0611\n",
      "Epoch 17, Batch 10/55, Loss: 0.0457\n",
      "Epoch 17, Batch 20/55, Loss: 0.0461\n",
      "Epoch 17, Batch 30/55, Loss: 0.0401\n",
      "Epoch 17, Batch 40/55, Loss: 0.0458\n",
      "Epoch 17, Batch 50/55, Loss: 0.0345\n",
      "Epoch 17 average loss: 0.0586\n",
      "Epoch 18, Batch 10/55, Loss: 0.0346\n",
      "Epoch 18, Batch 20/55, Loss: 0.0370\n",
      "Epoch 18, Batch 30/55, Loss: 0.0459\n",
      "Epoch 18, Batch 40/55, Loss: 0.2339\n",
      "Epoch 18, Batch 50/55, Loss: 0.0330\n",
      "Epoch 18 average loss: 0.0553\n",
      "Epoch 19, Batch 10/55, Loss: 0.0319\n",
      "Epoch 19, Batch 20/55, Loss: 0.0431\n",
      "Epoch 19, Batch 30/55, Loss: 0.0291\n",
      "Epoch 19, Batch 40/55, Loss: 0.0466\n",
      "Epoch 19, Batch 50/55, Loss: 0.0297\n",
      "Epoch 19 average loss: 0.0427\n",
      "Epoch 20, Batch 10/55, Loss: 0.0358\n",
      "Epoch 20, Batch 20/55, Loss: 0.0370\n",
      "Epoch 20, Batch 30/55, Loss: 0.0301\n",
      "Epoch 20, Batch 40/55, Loss: 0.0292\n",
      "Epoch 20, Batch 50/55, Loss: 0.0342\n",
      "Epoch 20 average loss: 0.0382\n",
      "Epoch 21, Batch 10/55, Loss: 0.0260\n",
      "Epoch 21, Batch 20/55, Loss: 0.0253\n",
      "Epoch 21, Batch 30/55, Loss: 0.0337\n",
      "Epoch 21, Batch 40/55, Loss: 0.0275\n",
      "Epoch 21, Batch 50/55, Loss: 0.0303\n",
      "Epoch 21 average loss: 0.0337\n",
      "Epoch 22, Batch 10/55, Loss: 0.0268\n",
      "Epoch 22, Batch 20/55, Loss: 0.0372\n",
      "Epoch 22, Batch 30/55, Loss: 0.0503\n",
      "Epoch 22, Batch 40/55, Loss: 0.0270\n",
      "Epoch 22, Batch 50/55, Loss: 0.0282\n",
      "Epoch 22 average loss: 0.0408\n",
      "Epoch 23, Batch 10/55, Loss: 0.0287\n",
      "Epoch 23, Batch 20/55, Loss: 0.0299\n",
      "Epoch 23, Batch 30/55, Loss: 0.0226\n",
      "Epoch 23, Batch 40/55, Loss: 0.0237\n",
      "Epoch 23, Batch 50/55, Loss: 0.0268\n",
      "Epoch 23 average loss: 0.0371\n",
      "Early stopping triggered\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training loop with early stopping\n",
    "model.train()\n",
    "patience = 2\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):  \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device).long()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Print every 10 batches\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i+1}/{len(train_dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model...\n",
      "\n",
      "Saving complete.\n"
     ]
    }
   ],
   "source": [
    "# Save the whole model\n",
    "print(\"\\nSaving model...\")\n",
    "torch.save(model, \"model/model_25epochs.pth\")\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"../model/model_state_25epochs.pth\")\n",
    "print(\"\\nSaving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n",
      "Test Accuracy: 0.9273\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"\\nEvaluating model...\")\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model...\n",
      "\n",
      "Model Load Completed\n"
     ]
    }
   ],
   "source": [
    "# Load saved model\n",
    "print(\"\\nLoading model...\")\n",
    "model = torch.load(\"../model/model_25epochs.pth\", weights_only=False)\n",
    "print(\"\\nModel Load Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict intent using the BERT model\n",
    "def predict_intent_bert(text):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "        predicted_label = label_encoder.inverse_transform([predicted.cpu().item()])[0]\n",
    "        \n",
    "    return predicted_label, confidence.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model with sample messages:\n",
      "--------------------------------------------------\n",
      "Message: I need to request time off for next week\n",
      "Predicted Intent: time_off_report\n",
      "Confidence: 0.98\n",
      "--------------------------------------------------\n",
      "Message: My computer keeps crashing and I can't work\n",
      "Predicted Intent: it_issue_report\n",
      "Confidence: 0.96\n",
      "--------------------------------------------------\n",
      "Message: I want to set up training for my team\n",
      "Predicted Intent: performance_review\n",
      "Confidence: 0.96\n",
      "--------------------------------------------------\n",
      "Message: I need access to the sales database\n",
      "Predicted Intent: access_request\n",
      "Confidence: 0.98\n",
      "--------------------------------------------------\n",
      "Message: I want to report the harm\n",
      "Predicted Intent: harassment_report\n",
      "Confidence: 0.78\n",
      "--------------------------------------------------\n",
      "Message: I would like to sign up for a training session on effective communication.\n",
      "Predicted Intent: training_request\n",
      "Confidence: 0.95\n",
      "--------------------------------------------------\n",
      "Message: Could you please do my performance review meeting?\n",
      "Predicted Intent: performance_review\n",
      "Confidence: 0.45\n",
      "--------------------------------------------------\n",
      "Message: I need access to the shared project drive; can you help me out?\n",
      "Predicted Intent: access_request\n",
      "Confidence: 0.98\n",
      "--------------------------------------------------\n",
      "Message: I’m planning to relocate to the New York office; what is the process?\n",
      "Predicted Intent: relocation_request\n",
      "Confidence: 0.98\n",
      "--------------------------------------------------\n",
      "Message: I want to report a safety incident that occurred in the warehouse.\n",
      "Predicted Intent: safety_incident_report\n",
      "Confidence: 0.95\n",
      "--------------------------------------------------\n",
      "Message: I’d like to request time off for next month due to a personal commitment.\n",
      "Predicted Intent: time_off_report\n",
      "Confidence: 0.98\n",
      "--------------------------------------------------\n",
      "Message: Can you help me enroll in the company benefits program, specifically health insurance?\n",
      "Predicted Intent: benefits_enrollment\n",
      "Confidence: 0.98\n",
      "--------------------------------------------------\n",
      "Message: I need to report an incident of harassment that I witnessed at work.\n",
      "Predicted Intent: harassment_report\n",
      "Confidence: 0.95\n",
      "--------------------------------------------------\n",
      "Message: How do I set my performance goals for the next quarter?\n",
      "Predicted Intent: performance_review\n",
      "Confidence: 0.97\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the model with sample messages\n",
    "test_messages = [\n",
    "    \"I need to request time off for next week\",\n",
    "    \"My computer keeps crashing and I can't work\",\n",
    "    \"I want to set up training for my team\",\n",
    "    \"I need access to the sales database\",\n",
    "    \"I want to report the harm\",\n",
    "    \"I would like to sign up for a training session on effective communication.\",\n",
    "    \"Could you please do my performance review meeting?\",\n",
    "    \"I need access to the shared project drive; can you help me out?\",\n",
    "    \"I’m planning to relocate to the New York office; what is the process?\",\n",
    "    \"I want to report a safety incident that occurred in the warehouse.\",\n",
    "    \"I’d like to request time off for next month due to a personal commitment.\",\n",
    "    \"Can you help me enroll in the company benefits program, specifically health insurance?\",\n",
    "    \"I need to report an incident of harassment that I witnessed at work.\",\n",
    "    \"How do I set my performance goals for the next quarter?\",\n",
    "    # \"My computer isn’t booting up properly; can you assist me with this issue?\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting model with sample messages:\")\n",
    "print(\"-\" * 50)\n",
    "for message in test_messages:\n",
    "    intent, confidence = predict_intent_bert(message)\n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Predicted Intent: {intent}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
